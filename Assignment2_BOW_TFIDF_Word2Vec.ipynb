{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ae0322e",
   "metadata": {},
   "source": [
    "# Assignment 2: Bag-of-Words, TF-IDF, and Word2Vec Embeddings\n",
    "\n",
    "This notebook demonstrates text vectorization techniques and word embeddings using scikit-learn and Gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c501ba",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb95a60",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer, TfidfVectorizer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdafe96",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb88819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"Machine learning is a subset of artificial intelligence\",\n",
    "    \"Deep learning uses neural networks with multiple layers\",\n",
    "    \"Natural language processing is used in chatbots and translation\",\n",
    "    \"Machine learning models require large amounts of training data\",\n",
    "    \"Python is the most popular language for machine learning\",\n",
    "    \"Data science involves statistics and machine learning\",\n",
    "    \"Neural networks are inspired by biological neurons\",\n",
    "    \"Text classification is a common natural language processing task\"\n",
    "]\n",
    "\n",
    "print(\"Sample Documents:\")\n",
    "print(\"-\" * 60)\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"{i}. {doc}\")\n",
    "\n",
    "print(f\"\\nTotal documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde4afdc",
   "metadata": {},
   "source": [
    "## 3. Bag-of-Words: Count Occurrence\n",
    "\n",
    "Create a bag-of-words model with raw word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ed21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CountVectorizer (Bag-of-Words with count occurrence)\n",
    "count_vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
    "bow_count_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Bag-of-Words: Count Occurrence\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Matrix shape: {bow_count_matrix.shape}\")\n",
    "print(f\"Number of documents: {bow_count_matrix.shape[0]}\")\n",
    "print(f\"Number of unique words: {bow_count_matrix.shape[1]}\")\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "print(f\"\\nVocabulary (first 20 words): {feature_names[:20]}\")\n",
    "\n",
    "# Convert to dense array for better visualization\n",
    "bow_count_dense = bow_count_matrix.toarray()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "bow_df = pd.DataFrame(bow_count_dense, columns=feature_names)\n",
    "print(\"\\nBag-of-Words Count Matrix (first 5 documents, first 15 words):\")\n",
    "print(bow_df.iloc[:5, :15])\n",
    "\n",
    "# Show word counts for first document\n",
    "print(\"\\nWord counts in first document:\")\n",
    "first_doc_counts = bow_df.iloc[0].sort_values(ascending=False)\n",
    "print(first_doc_counts[first_doc_counts > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c9f61b",
   "metadata": {},
   "source": [
    "## 4. Bag-of-Words: Normalized Count Occurrence\n",
    "\n",
    "Normalize BoW vectors using term frequency normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the BoW counts using L2 normalization (Term Frequency)\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# L2 normalization (divide each row by its L2 norm)\n",
    "bow_normalized_l2 = normalize(bow_count_matrix, norm='l2')\n",
    "bow_normalized_l2_dense = bow_normalized_l2.toarray()\n",
    "\n",
    "# L1 normalization (divide by the sum of absolute values)\n",
    "bow_normalized_l1 = normalize(bow_count_matrix, norm='l1')\n",
    "bow_normalized_l1_dense = bow_normalized_l1.toarray()\n",
    "\n",
    "print(\"Bag-of-Words: Normalized Count Occurrence\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create DataFrames for normalized BoW\n",
    "bow_norm_df = pd.DataFrame(bow_normalized_l2_dense, columns=feature_names)\n",
    "\n",
    "print(\"\\nL2 Normalized BoW (first 5 documents, first 10 words):\")\n",
    "print(bow_norm_df.iloc[:5, :10])\n",
    "\n",
    "print(\"\\n\\nComparison of raw counts vs normalized (Document 1):\")\n",
    "print(\"-\" * 60)\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Word': feature_names[:15],\n",
    "    'Raw Count': bow_count_dense[0, :15],\n",
    "    'L2 Normalized': bow_normalized_l2_dense[0, :15],\n",
    "    'L1 Normalized': bow_normalized_l1_dense[0, :15]\n",
    "})\n",
    "print(comparison_df[comparison_df['Raw Count'] > 0])\n",
    "\n",
    "print(\"\\n\\nL2 Normalization (Term Frequency) - Statistical Summary:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Min normalized value: {bow_normalized_l2_dense[bow_normalized_l2_dense > 0].min():.6f}\")\n",
    "print(f\"Max normalized value: {bow_normalized_l2_dense.max():.6f}\")\n",
    "print(f\"Mean normalized value: {bow_normalized_l2_dense[bow_normalized_l2_dense > 0].mean():.6f}\")\n",
    "\n",
    "# Show L2 norm (should be 1 for each document)\n",
    "print(\"\\nL2 norm for each document (should be ~1.0):\")\n",
    "for i in range(len(documents)):\n",
    "    l2_norm = np.linalg.norm(bow_normalized_l2_dense[i])\n",
    "    print(f\"Document {i+1}: {l2_norm:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5608dceb",
   "metadata": {},
   "source": [
    "## 5. TF-IDF Vectorization\n",
    "\n",
    "Compute TF-IDF (Term Frequency-Inverse Document Frequency) scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6534720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"TF-IDF Vectorization\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Number of documents: {tfidf_matrix.shape[0]}\")\n",
    "print(f\"Number of unique words: {tfidf_matrix.shape[1]}\")\n",
    "\n",
    "# Get feature names\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to dense array\n",
    "tfidf_dense = tfidf_matrix.toarray()\n",
    "\n",
    "# Create DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_dense, columns=tfidf_feature_names)\n",
    "\n",
    "print(\"\\nTF-IDF Matrix (first 5 documents, first 10 words):\")\n",
    "print(tfidf_df.iloc[:5, :10])\n",
    "\n",
    "print(\"\\n\\nTop 10 Most Important Words (Highest TF-IDF scores) in Each Document:\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(len(documents)):\n",
    "    print(f\"\\nDocument {i+1}: {documents[i][:50]}...\")\n",
    "    doc_tfidf = tfidf_df.iloc[i]\n",
    "    top_words = doc_tfidf.nlargest(10)\n",
    "    for word, score in top_words.items():\n",
    "        if score > 0:\n",
    "            print(f\"  {word:<20} : {score:.4f}\")\n",
    "\n",
    "# Show IDF values\n",
    "print(\"\\n\\nInverse Document Frequency (IDF) Values:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"IDF = log(total_docs / docs_containing_term)\")\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "\n",
    "idf_values = tfidf_vectorizer.idf_\n",
    "idf_dict = dict(zip(tfidf_feature_names, idf_values))\n",
    "# Sort by IDF value\n",
    "sorted_idf = sorted(idf_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 15 words by IDF (most discriminative):\")\n",
    "for word, idf in sorted_idf[:15]:\n",
    "    print(f\"  {word:<20} : {idf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6078fd9b",
   "metadata": {},
   "source": [
    "## 6. Comparison: BoW vs Normalized BoW vs TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a7625",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparison of Vectorization Methods\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compare for document 1 and first 10 words\n",
    "doc_idx = 0\n",
    "comparison_words = tfidf_feature_names[:15]\n",
    "word_indices = [np.where(tfidf_feature_names == word)[0][0] for word in comparison_words]\n",
    "\n",
    "print(f\"\\nDocument 1: {documents[0]}\")\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(f\"{'Word':<20} | {'BoW Count':<15} | {'BoW Normalized':<20} | {'TF-IDF':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for word, idx in zip(comparison_words, word_indices):\n",
    "    bow_count = bow_count_dense[doc_idx, idx]\n",
    "    bow_norm = bow_normalized_l2_dense[doc_idx, idx]\n",
    "    tfidf = tfidf_dense[doc_idx, idx]\n",
    "    print(f\"{word:<20} | {bow_count:<15.0f} | {bow_norm:<20.6f} | {tfidf:<15.6f}\")\n",
    "\n",
    "print(\"\\n\\nKey Differences:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"1. BoW Count: Raw word frequencies - favors longer documents\")\n",
    "print(\"2. BoW Normalized: Term frequencies - accounts for document length\")\n",
    "print(\"3. TF-IDF: Weights terms by importance - reduces impact of common words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba4d1d",
   "metadata": {},
   "source": [
    "## 7. Word2Vec: Training the Model\n",
    "\n",
    "Train a Word2Vec model using Gensim on the tokenized corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46926482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize documents for Word2Vec\n",
    "tokenized_docs = []\n",
    "for doc in documents:\n",
    "    tokens = simple_preprocess(doc)  # Converts to lowercase and tokenizes\n",
    "    tokenized_docs.append(tokens)\n",
    "\n",
    "print(\"Tokenized Documents:\")\n",
    "print(\"=\" * 60)\n",
    "for i, tokens in enumerate(tokenized_docs):\n",
    "    print(f\"Doc {i+1}: {tokens}\")\n",
    "\n",
    "# Train Word2Vec model (Skip-gram)\n",
    "print(\"\\n\\nTraining Word2Vec Model (Skip-gram)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "w2v_model_skipgram = Word2Vec(sentences=tokenized_docs, \n",
    "                               vector_size=100,      # Dimension of word vectors\n",
    "                               window=5,              # Context window size\n",
    "                               min_count=1,           # Minimum word frequency\n",
    "                               workers=4,             # Number of worker threads\n",
    "                               sg=1)                  # 1 = Skip-gram, 0 = CBOW\n",
    "\n",
    "print(f\"Vocabulary size: {len(w2v_model_skipgram.wv)}\")\n",
    "print(f\"Vector size: {w2v_model_skipgram.vector_size}\")\n",
    "\n",
    "# Get word vectors\n",
    "print(\"\\n\\nWord Vectors (first 5 words):\")\n",
    "for word in list(w2v_model_skipgram.wv.index_to_key)[:5]:\n",
    "    vector = w2v_model_skipgram.wv[word]\n",
    "    print(f\"{word}: {vector[:5]}... (showing first 5 dimensions)\")\n",
    "\n",
    "# Train Word2Vec model (CBOW)\n",
    "print(\"\\n\\nTraining Word2Vec Model (CBOW - Continuous Bag of Words)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "w2v_model_cbow = Word2Vec(sentences=tokenized_docs,\n",
    "                           vector_size=100,\n",
    "                           window=5,\n",
    "                           min_count=1,\n",
    "                           workers=4,\n",
    "                           sg=0)  # 0 = CBOW\n",
    "\n",
    "print(f\"Vocabulary size: {len(w2v_model_cbow.wv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d16193",
   "metadata": {},
   "source": [
    "## 8. Word2Vec: Word Similarity\n",
    "\n",
    "Find similar words and calculate similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c92f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most similar words\n",
    "print(\"Most Similar Words (Skip-gram model):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_words = ['learning', 'neural', 'processing']\n",
    "\n",
    "for word in test_words:\n",
    "    if word in w2v_model_skipgram.wv:\n",
    "        print(f\"\\nWords most similar to '{word}':\")\n",
    "        similar_words = w2v_model_skipgram.wv.most_similar(word, topn=5)\n",
    "        for similar_word, similarity in similar_words:\n",
    "            print(f\"  {similar_word:<20} : {similarity:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\nWord '{word}' not in vocabulary\")\n",
    "\n",
    "# Calculate similarity between word pairs\n",
    "print(\"\\n\\nWord-to-Word Similarity:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "word_pairs = [\n",
    "    ('machine', 'learning'),\n",
    "    ('neural', 'networks'),\n",
    "    ('python', 'language'),\n",
    "    ('data', 'statistics'),\n",
    "]\n",
    "\n",
    "for word1, word2 in word_pairs:\n",
    "    if word1 in w2v_model_skipgram.wv and word2 in w2v_model_skipgram.wv:\n",
    "        similarity = w2v_model_skipgram.wv.similarity(word1, word2)\n",
    "        print(f\"Similarity('{word1}', '{word2}'): {similarity:.4f}\")\n",
    "    else:\n",
    "        print(f\"One of the words '{word1}' or '{word2}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeed0a37",
   "metadata": {},
   "source": [
    "## 9. Word2Vec: Document Representation\n",
    "\n",
    "Create document embeddings by averaging word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26550f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document embeddings by averaging word vectors\n",
    "def get_doc_embedding(doc_tokens, model):\n",
    "    \"\"\"Get document embedding by averaging word vectors\"\"\"\n",
    "    vectors = []\n",
    "    for token in doc_tokens:\n",
    "        if token in model.wv:\n",
    "            vectors.append(model.wv[token])\n",
    "    \n",
    "    if len(vectors) == 0:\n",
    "        # Return zero vector if no words found\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Get document embeddings for all documents\n",
    "print(\"Document Embeddings (averaged word vectors)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "doc_embeddings = []\n",
    "for i, tokens in enumerate(tokenized_docs):\n",
    "    embedding = get_doc_embedding(tokens, w2v_model_skipgram)\n",
    "    doc_embeddings.append(embedding)\n",
    "    print(f\"Document {i+1} embedding shape: {embedding.shape}\")\n",
    "    print(f\"  First 10 dimensions: {embedding[:10]}\")\n",
    "\n",
    "doc_embeddings = np.array(doc_embeddings)\n",
    "\n",
    "# Calculate document-to-document similarity\n",
    "print(\"\\n\\nDocument-to-Document Similarity (Cosine similarity):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(doc_embeddings)\n",
    "\n",
    "# Create a nice display\n",
    "sim_df = pd.DataFrame(similarity_matrix)\n",
    "print(\"\\nDocument Similarity Matrix:\")\n",
    "print(sim_df.round(3))\n",
    "\n",
    "# Find most similar document pairs\n",
    "print(\"\\n\\nMost Similar Document Pairs:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Get upper triangle indices to avoid duplicates\n",
    "upper_triangle = np.triu_indices_from(similarity_matrix, k=1)\n",
    "similarities = []\n",
    "for i, j in zip(upper_triangle[0], upper_triangle[1]):\n",
    "    similarities.append((i, j, similarity_matrix[i, j]))\n",
    "\n",
    "# Sort by similarity\n",
    "similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "for doc1_idx, doc2_idx, sim in similarities[:5]:\n",
    "    print(f\"\\nDocument {doc1_idx+1} & {doc2_idx+1}: {sim:.4f}\")\n",
    "    print(f\"  Doc {doc1_idx+1}: {documents[doc1_idx][:50]}...\")\n",
    "    print(f\"  Doc {doc2_idx+1}: {documents[doc2_idx][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b369c",
   "metadata": {},
   "source": [
    "## 10. Visualization: Word2Vec Embeddings\n",
    "\n",
    "Visualize high-dimensional embeddings using dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word embeddings using t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Get all word vectors\n",
    "word_vectors = w2v_model_skipgram.wv.vectors\n",
    "words = w2v_model_skipgram.wv.index_to_key\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction (100D -> 2D)\n",
    "print(\"Reducing dimensionality using t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(words)-1))\n",
    "vectors_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.6, s=50)\n",
    "\n",
    "# Add labels for words\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), \n",
    "                fontsize=9, ha='center')\n",
    "\n",
    "plt.title(\"Word2Vec Embeddings - t-SNE Visualization\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa310e8e",
   "metadata": {},
   "source": [
    "## 11. Summary and Comparison\n",
    "\n",
    "Summary of all techniques demonstrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcc2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SUMMARY OF TEXT VECTORIZATION TECHNIQUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = \"\"\"\n",
    "1. BAG-OF-WORDS (BoW - Count Occurrence):\n",
    "   - Representation: Count of each word in a document\n",
    "   - Pros: Simple, interpretable, fast\n",
    "   - Cons: Loses word order, large sparse vectors\n",
    "   - Use case: Baseline approach, document classification\n",
    "   \n",
    "2. BAG-OF-WORDS (Normalized Count):\n",
    "   - Representation: Term Frequency (TF) - normalized word counts\n",
    "   - Pros: Accounts for document length, normalized values [0,1]\n",
    "   - Cons: Still loses word order, ignores word importance\n",
    "   - Use case: When document length varies significantly\n",
    "   \n",
    "3. TF-IDF (Term Frequency-Inverse Document Frequency):\n",
    "   - Representation: TF * IDF - weights terms by importance\n",
    "   - Formula: TF-IDF = (word_count/total_words) * log(total_docs/docs_with_word)\n",
    "   - Pros: Reduces impact of common words, better for info retrieval\n",
    "   - Cons: Still sparse, context-independent\n",
    "   - Use case: Information retrieval, search engines, document similarity\n",
    "   \n",
    "4. WORD2VEC:\n",
    "   - Representation: Dense word embeddings (100-300 dimensions)\n",
    "   - Types: Skip-gram (predicts context from word) or CBOW (predicts word from context)\n",
    "   - Pros: Captures semantic relationships, dense vectors, context-aware\n",
    "   - Cons: Requires more training data, harder to interpret\n",
    "   - Use case: Similarity search, analogies, downstream NLP tasks\n",
    "\n",
    "COMPARISON TABLE:\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "comparison_table = pd.DataFrame({\n",
    "    'Technique': ['BoW Count', 'BoW Normalized', 'TF-IDF', 'Word2Vec'],\n",
    "    'Dimensionality': ['Sparse (Vocab Size)', 'Sparse (Vocab Size)', 'Sparse (Vocab Size)', 'Dense (100-300)'],\n",
    "    'Captures Order': ['No', 'No', 'No', 'Yes (context)'],\n",
    "    'Computational Cost': ['Low', 'Low', 'Medium', 'High'],\n",
    "    'Semantic Info': ['No', 'No', 'Weak', 'Strong'],\n",
    "    'Best For': ['Baseline', 'Classification', 'Info Retrieval', 'Deep Learning']\n",
    "})\n",
    "\n",
    "print(comparison_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONCLUSION:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\"\"\n",
    "- Choose BoW for simple, interpretable baselines\n",
    "- Use TF-IDF for information retrieval and document ranking\n",
    "- Use Word2Vec for semantic similarity and advanced NLP tasks\n",
    "- Combine multiple techniques for best results in real-world applications\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
