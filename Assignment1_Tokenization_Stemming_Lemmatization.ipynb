{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c1935cc",
   "metadata": {},
   "source": [
    "# Assignment 1: NLTK Tokenization, Stemming, and Lemmatization\n",
    "\n",
    "This notebook demonstrates various tokenization techniques, stemming methods, and lemmatization using the NLTK library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15b663",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "798e75c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import (WhitespaceTokenizer, WordPunctTokenizer, \n",
    "                           TreebankWordTokenizer, TweetTokenizer, MWETokenizer)\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('wordnet_ic', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "print(\"All required libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f17c25",
   "metadata": {},
   "source": [
    "## 2. Sample Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1305571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text:\n",
      "Natural Language Processing (NLP) is fascinating! \n",
      "                It's used in machine learning, deep learning, and AI. \n",
      "                Dr. Smith is working on NLP applications.\n",
      "\n",
      "Sample Tweet:\n",
      "I'm loving this! @NLPenthusiasts #Python #MachineLearning :) Check out https://example.com\n"
     ]
    }
   ],
   "source": [
    "# Sample texts for demonstration\n",
    "sample_text = \"\"\"Natural Language Processing (NLP) is fascinating! \n",
    "                It's used in machine learning, deep learning, and AI. \n",
    "                Dr. Smith is working on NLP applications.\"\"\"\n",
    "\n",
    "sample_tweet = \"I'm loving this! @NLPenthusiasts #Python #MachineLearning :) Check out https://example.com\"\n",
    "\n",
    "print(\"Sample Text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nSample Tweet:\")\n",
    "print(sample_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09e705",
   "metadata": {},
   "source": [
    "## 3. Whitespace Tokenization\n",
    "\n",
    "Splits text on whitespace characters only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fa241e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokenization:\n",
      "Number of tokens: 22\n",
      "Tokens: ['Natural', 'Language', 'Processing', '(NLP)', 'is', 'fascinating!', \"It's\", 'used', 'in', 'machine', 'learning,', 'deep', 'learning,', 'and', 'AI.', 'Dr.', 'Smith', 'is', 'working', 'on', 'NLP', 'applications.']\n",
      "\n",
      "First 10 tokens: ['Natural', 'Language', 'Processing', '(NLP)', 'is', 'fascinating!', \"It's\", 'used', 'in', 'machine']\n"
     ]
    }
   ],
   "source": [
    "ws_tokenizer = WhitespaceTokenizer()\n",
    "ws_tokens = ws_tokenizer.tokenize(sample_text)\n",
    "\n",
    "print(\"Whitespace Tokenization:\")\n",
    "print(f\"Number of tokens: {len(ws_tokens)}\")\n",
    "print(f\"Tokens: {ws_tokens}\")\n",
    "print(f\"\\nFirst 10 tokens: {ws_tokens[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555190c",
   "metadata": {},
   "source": [
    "## 4. Punctuation-based Tokenization\n",
    "\n",
    "Separates words from punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a878672d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation-based Tokenization (WordPunctTokenizer):\n",
      "Number of tokens: 32\n",
      "Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'fascinating', '!', 'It', \"'\", 's', 'used', 'in', 'machine', 'learning', ',', 'deep', 'learning', ',', 'and', 'AI', '.', 'Dr', '.', 'Smith', 'is', 'working', 'on', 'NLP', 'applications', '.']\n",
      "\n",
      "First 15 tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'fascinating', '!', 'It', \"'\", 's', 'used', 'in', 'machine']\n"
     ]
    }
   ],
   "source": [
    "wp_tokenizer = WordPunctTokenizer()\n",
    "wp_tokens = wp_tokenizer.tokenize(sample_text)\n",
    "\n",
    "print(\"Punctuation-based Tokenization (WordPunctTokenizer):\")\n",
    "print(f\"Number of tokens: {len(wp_tokens)}\")\n",
    "print(f\"Tokens: {wp_tokens}\")\n",
    "print(f\"\\nFirst 15 tokens: {wp_tokens[:15]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9a5fac",
   "metadata": {},
   "source": [
    "## 5. Treebank Tokenization\n",
    "\n",
    "Follows Penn Treebank tokenization rules, handling contractions specially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f50137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treebank Tokenization:\n",
      "Number of tokens: 29\n",
      "Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'fascinating', '!', 'It', \"'s\", 'used', 'in', 'machine', 'learning', ',', 'deep', 'learning', ',', 'and', 'AI.', 'Dr.', 'Smith', 'is', 'working', 'on', 'NLP', 'applications', '.']\n",
      "\n",
      "Notice how contractions are handled: It's -> It + 's\n",
      "\n",
      "Comparison text: It's a beautiful day. I've never seen such beauty.\n",
      "Treebank tokens: ['It', \"'s\", 'a', 'beautiful', 'day.', 'I', \"'ve\", 'never', 'seen', 'such', 'beauty', '.']\n"
     ]
    }
   ],
   "source": [
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "treebank_tokens = treebank_tokenizer.tokenize(sample_text)\n",
    "\n",
    "print(\"Treebank Tokenization:\")\n",
    "print(f\"Number of tokens: {len(treebank_tokens)}\")\n",
    "print(f\"Tokens: {treebank_tokens}\")\n",
    "print(f\"\\nNotice how contractions are handled: It's -> It + 's\")\n",
    "# Show comparison with contractions\n",
    "comparison_text = \"It's a beautiful day. I've never seen such beauty.\"\n",
    "print(f\"\\nComparison text: {comparison_text}\")\n",
    "print(f\"Treebank tokens: {treebank_tokenizer.tokenize(comparison_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c61a5",
   "metadata": {},
   "source": [
    "## 6. Tweet Tokenization\n",
    "\n",
    "Handles social media text with emoticons, hashtags, mentions, and URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "014d0386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Tokenization:\n",
      "Number of tokens: 11\n",
      "Tokens: [\"I'm\", 'loving', 'this', '!', '@NLPenthusiasts', '#Python', '#MachineLearning', ':)', 'Check', 'out', 'https://example.com']\n",
      "\n",
      "Notice: Mentions (@), hashtags (#), URLs, and emoticons (:)) are preserved as separate tokens\n"
     ]
    }
   ],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(sample_tweet)\n",
    "\n",
    "print(\"Tweet Tokenization:\")\n",
    "print(f\"Number of tokens: {len(tweet_tokens)}\")\n",
    "print(f\"Tokens: {tweet_tokens}\")\n",
    "print(f\"\\nNotice: Mentions (@), hashtags (#), URLs, and emoticons (:)) are preserved as separate tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c073b5",
   "metadata": {},
   "source": [
    "## 7. Multi-Word Expression (MWE) Tokenization\n",
    "\n",
    "Treats multi-word expressions as single tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af8a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MWE Tokenizer - treats multi-word expressions as single tokens\n",
    "mwe_text = \"New York is a major city. Machine learning is fascinating.\"\n",
    "\n",
    "# Define multi-word expressions\n",
    "mwe_tokenizer = MWETokenizer([('New', 'York'), ('Machine', 'learning')])\n",
    "\n",
    "# First tokenize the text\n",
    "basic_tokens = mwe_text.split()\n",
    "print(\"Basic tokenization (before MWE):\")\n",
    "print(basic_tokens)\n",
    "\n",
    "# Then apply MWE\n",
    "from nltk.tokenize import word_tokenize\n",
    "basic_tokens_nltk = word_tokenize(mwe_text)\n",
    "mwe_tokens = mwe_tokenizer.tokenize(basic_tokens_nltk)\n",
    "\n",
    "print(\"\\nMWE Tokenization (after MWE):\")\n",
    "print(f\"Tokens: {mwe_tokens}\")\n",
    "print(f\"Notice how 'New York' and 'Machine learning' are treated as single tokens using underscore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a3a7c",
   "metadata": {},
   "source": [
    "## 8. Porter Stemmer\n",
    "\n",
    "Reduces words to their root form using the Porter stemming algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36360de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Sample words to stem\n",
    "words_to_stem = ['running', 'runs', 'ran', 'runner', 'easily', 'fairly',\n",
    "                  'working', 'worked', 'works', 'organization', 'organize',\n",
    "                  'organized', 'organizes', 'programming', 'programs']\n",
    "\n",
    "print(\"Porter Stemmer Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Original Word':<20} | {'Stemmed Word':<20}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "stemmed_porter = []\n",
    "for word in words_to_stem:\n",
    "    stem = porter_stemmer.stem(word)\n",
    "    stemmed_porter.append((word, stem))\n",
    "    print(f\"{word:<20} | {stem:<20}\")\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- 'running', 'runs', 'ran' all stem to 'run'\")\n",
    "print(\"- 'organize', 'organized', 'organizing' all stem to 'organ'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635df1a",
   "metadata": {},
   "source": [
    "## 9. Snowball Stemmer\n",
    "\n",
    "Universal stemmer supporting multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a90f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowball Stemmer\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "print(\"Snowball Stemmer Results (English):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Original Word':<20} | {'Porter':<15} | {'Snowball':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for word in words_to_stem:\n",
    "    porter_stem = porter_stemmer.stem(word)\n",
    "    snowball_stem = snowball_stemmer.stem(word)\n",
    "    print(f\"{word:<20} | {porter_stem:<15} | {snowball_stem:<15}\")\n",
    "\n",
    "# Demonstrate Snowball with different languages\n",
    "print(\"\\n\\nSnowball Stemmer - Multilingual Support:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "languages = ['english', 'french', 'spanish', 'german']\n",
    "test_words = {\n",
    "    'english': ['running', 'organized'],\n",
    "    'french': ['exÃ©cution', 'organisation'],\n",
    "    'spanish': ['corriendo', 'organizado'],\n",
    "    'german': ['laufen', 'organisiert']\n",
    "}\n",
    "\n",
    "for lang in languages:\n",
    "    if lang in test_words:\n",
    "        stemmer = SnowballStemmer(lang)\n",
    "        print(f\"\\n{lang.upper()}:\")\n",
    "        for word in test_words[lang]:\n",
    "            print(f\"  {word} -> {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dcf45",
   "metadata": {},
   "source": [
    "## 10. Lemmatization with WordNet\n",
    "\n",
    "Convert words to their base form (lemma) using WordNet database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af59921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Helper function to get POS tag for lemmatization\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert Treebank POS tags to WordNet POS tags\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Sample text for lemmatization\n",
    "text_for_lemma = \"The cars are running quickly. He is running away. They have run.\"\n",
    "\n",
    "# Tokenize and tag POS\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text_for_lemma)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(\"Lemmatization with POS Tagging:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Token':<15} | {'POS Tag':<10} | {'Lemma':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "lemmatized_words = []\n",
    "for token, pos in pos_tags:\n",
    "    wordnet_pos = get_wordnet_pos(pos)\n",
    "    if wordnet_pos is None:\n",
    "        lemma = lemmatizer.lemmatize(token)\n",
    "    else:\n",
    "        lemma = lemmatizer.lemmatize(token, pos=wordnet_pos)\n",
    "    lemmatized_words.append(lemma)\n",
    "    print(f\"{token:<15} | {pos:<10} | {lemma:<15}\")\n",
    "\n",
    "print(f\"\\nOriginal text: {text_for_lemma}\")\n",
    "print(f\"Lemmatized text: {' '.join(lemmatized_words)}\")\n",
    "\n",
    "# Comparison: Lemmatization vs Stemming\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"Lemmatization vs Stemming Comparison:\")\n",
    "print(\"=\"*60)\n",
    "comparison_words = ['running', 'runs', 'ran', 'easily', 'better', 'was', 'is', 'am']\n",
    "print(f\"{'Word':<15} | {'Lemma':<15} | {'Porter Stem':<15} | {'Snowball Stem':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for word in comparison_words:\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    porter = porter_stemmer.stem(word)\n",
    "    snowball = snowball_stemmer.stem(word)\n",
    "    print(f\"{word:<15} | {lemma:<15} | {porter:<15} | {snowball:<15}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
